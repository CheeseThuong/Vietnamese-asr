# Vietnamese Speech Recognition (ASR) Project
# Nh·∫≠n d·∫°ng gi·ªçng n√≥i ti·∫øng Vi·ªát

ƒê·ªÅ t√†i: **Nh·∫≠n d·∫°ng ti·∫øng n√≥i Ti·∫øng Vi·ªát** s·ª≠ d·ª•ng Wav2Vec 2.0

##  T·ªïng quan d·ª± √°n

D·ª± √°n n√†y x√¢y d·ª±ng h·ªá th·ªëng nh·∫≠n d·∫°ng gi·ªçng n√≥i ti·∫øng Vi·ªát (ASR) s·ª≠ d·ª•ng ki·∫øn tr√∫c Wav2Vec 2.0, ƒë∆∞·ª£c fine-tune tr√™n d·ªØ li·ªáu VIVOS v√† VinBigData. H·ªá th·ªëng bao g·ªìm:

- Fine-tuning Wav2Vec2 model
- T√≠ch h·ª£p Language Model ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c
- Web application ƒë·ªÉ upload file ho·∫∑c ghi √¢m tr·ª±c ti·∫øp
- C√°c c√¥ng c·ª• t·ªëi ∆∞u h√≥a hi·ªáu su·∫•t (BitNet quantization, ONNX export)

##  M·ª•c ti√™u

- Chuy·ªÉn ƒë·ªïi gi·ªçng n√≥i ti·∫øng Vi·ªát (ƒëa v√πng mi·ªÅn) th√†nh vƒÉn b·∫£n
- ƒê·∫°t WER (Word Error Rate) < 10% tr√™n test set
- ·ª®ng d·ª•ng web th√¢n thi·ªán, d·ªÖ s·ª≠ d·ª•ng
- T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t inference

##  Dataset

- **VIVOS**: ~15 gi·ªù ghi √¢m ch·∫•t l∆∞·ª£ng cao
- **VinBigData VLSP 2020**: Dataset ti·∫øng Vi·ªát quy m√¥ l·ªõn

##  H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

### Option 1: Training tr√™n Google Colab (Khuy·∫øn ngh·ªã - GPU mi·ªÖn ph√≠)

1. **Chu·∫©n b·ªã dataset:**
   ```bash
   python prepare_vivos.py
   ```

2. **Upload dataset l√™n Google Drive:**
   - T·∫°o folder: `MyDrive/VietnameseASR/data/`
   - Upload 3 files t·ª´ `processed_data_vivos/`

3. **M·ªü Colab notebook:**
   - Upload [colab_train.ipynb](colab_train.ipynb) l√™n Colab
   - Runtime ‚Üí Change runtime type ‚Üí GPU
   - Ch·∫°y t·ª´ng cell

    **Chi ti·∫øt:** Xem [colab_setup.md](colab_setup.md)

### Option 2: Training tr√™n m√°y local (CPU - m·∫•t ~6 ng√†y)

### 1. C√†i ƒë·∫∑t m√¥i tr∆∞·ªùng

```bash
# T·∫°o virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. X·ª≠ l√Ω v√† g·ªôp d·ªØ li·ªáu

```bash
python prepare_dataset.py
```

Script n√†y s·∫Ω:
- ƒê·ªçc d·ªØ li·ªáu t·ª´ VIVOS v√† VinBigData
- Chu·∫©n h√≥a format
- G·ªôp v√† chia th√†nh train/validation/test sets
- L∆∞u v√†o th∆∞ m·ª•c `processed_data/`

### 3. Preprocessing d·ªØ li·ªáu

```bash
python data_preprocessing.py
```

Script n√†y s·∫Ω:
- T·∫°o vocabulary t·ª´ d·ªØ li·ªáu training
- T·∫°o Wav2Vec2Processor
- Chu·∫©n b·ªã d·ªØ li·ªáu cho training

### 4. Training model

```bash
python train_wav2vec2.py
```

Training s·∫Ω:
- Fine-tune Wav2Vec2 model (pre-trained ho·∫∑c from scratch)
- √Åp d·ª•ng BitNet quantization (n·∫øu c√≥)
- ƒê√°nh gi√° tr√™n validation set
- L∆∞u model v√†o `models/wav2vec2-vietnamese-asr/`

**C·∫•u h√¨nh training** trong file:
- `pretrained_model`: Model pre-trained (m·∫∑c ƒë·ªãnh: nguyenvulebinh/wav2vec2-base-vietnamese-250h)
- `num_train_epochs`: 30
- `batch_size`: 8
- `learning_rate`: 3e-4

### 5. Build Language Model

```bash
python language_model.py
```

Script n√†y s·∫Ω:
- Chu·∫©n b·ªã corpus t·ª´ training data
- Build 5-gram KenLM
- L∆∞u v√†o `language_models/`

**L∆∞u √Ω**: C·∫ßn c√†i ƒë·∫∑t KenLM:
```bash
pip install https://github.com/kpu/kenlm/archive/master.zip
```

### 6. Evaluation

```bash
python run_evaluation.py
```

ƒê√°nh gi√° model tr√™n test set v·ªõi:
- WER (Word Error Rate)
- CER (Character Error Rate)
- So s√°nh v·ªõi/kh√¥ng Language Model

### 7. Ch·∫°y Web Application

```bash
# Start API server
python api_server.py

# Ho·∫∑c d√πng uvicorn
uvicorn api_server:app --reload --host 0.0.0.0 --port 8000
```

Truy c·∫≠p: http://localhost:8000/app

Web app c√≥ t√≠nh nƒÉng:
- ‚úÖ Upload file audio (WAV, MP3, FLAC, etc.)
- ‚úÖ Ghi √¢m tr·ª±c ti·∫øp t·ª´ microphone
- ‚úÖ Hi·ªÉn th·ªã k·∫øt qu·∫£ real-time
- ‚úÖ Toggle Language Model on/off

### 8. T·ªëi ∆∞u h√≥a hi·ªáu su·∫•t

```bash
python optimization.py
```

Script n√†y s·∫Ω:
- Apply quantization
- Export sang ONNX format
- Benchmark inference performance
- Profile v·ªõi PyFlame (n·∫øu c√≥)

##  C·∫•u tr√∫c th∆∞ m·ª•c

```
‚îú‚îÄ‚îÄ Data/                           # Raw datasets
‚îÇ   ‚îú‚îÄ‚îÄ vivos/
‚îÇ   ‚îî‚îÄ‚îÄ Data/ (VinBigData)
‚îú‚îÄ‚îÄ processed_data/                 # Processed datasets
‚îÇ   ‚îú‚îÄ‚îÄ train.jsonl
‚îÇ   ‚îú‚îÄ‚îÄ validation.jsonl
‚îÇ   ‚îî‚îÄ‚îÄ test.jsonl
‚îú‚îÄ‚îÄ models/                         # Trained models
‚îÇ   ‚îî‚îÄ‚îÄ wav2vec2-vietnamese-asr/
‚îÇ       ‚îú‚îÄ‚îÄ final_model/
‚îÇ       ‚îî‚îÄ‚îÄ model.onnx
‚îú‚îÄ‚îÄ language_models/                # Language models
‚îÇ   ‚îú‚îÄ‚îÄ vietnamese_5gram.bin
‚îÇ   ‚îî‚îÄ‚îÄ lm_corpus.txt
‚îú‚îÄ‚îÄ static/                         # Web UI
‚îÇ   ‚îî‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ results/                        # Evaluation results
‚îÇ   ‚îú‚îÄ‚îÄ final_results.json
‚îÇ   ‚îî‚îÄ‚îÄ predictions_with_lm.json
‚îú‚îÄ‚îÄ prepare_dataset.py              # Dataset preparation
‚îú‚îÄ‚îÄ data_preprocessing.py           # Data preprocessing
‚îú‚îÄ‚îÄ train_wav2vec2.py              # Training script
‚îú‚îÄ‚îÄ language_model.py              # LM building
‚îú‚îÄ‚îÄ run_evaluation.py              # Evaluation
‚îú‚îÄ‚îÄ api_server.py                  # FastAPI backend
‚îú‚îÄ‚îÄ optimization.py                # Performance optimization
‚îî‚îÄ‚îÄ requirements.txt               # Dependencies
```

##  API Endpoints

### GET /health
Ki·ªÉm tra tr·∫°ng th√°i server

### POST /transcribe
Transcribe audio file

**Parameters:**
- `file`: Audio file (multipart/form-data)
- `use_lm`: Boolean (s·ª≠ d·ª•ng Language Model)

**Response:**
```json
{
  "text": "vƒÉn b·∫£n nh·∫≠n d·∫°ng ƒë∆∞·ª£c",
  "processing_time": 1.23,
  "language_model_used": true,
  "audio_duration": 5.4
}
```

### GET /model-info
Th√¥ng tin v·ªÅ model ƒë√£ load

## üìä K·∫øt qu·∫£

### Baseline (Greedy Decoding)
- WER: ~12-15%
- CER: ~6-8%

### With Language Model
- WER: ~8-10% (c·∫£i thi·ªán ~20-30%)
- CER: ~4-6% (c·∫£i thi·ªán ~20-30%)

### Performance
- Inference time: ~0.5-1s cho audio 5s
- Model size: ~400MB (full) / ~100MB (quantized)
- Real-time factor (RTF): < 0.2

## üõ†Ô∏è T·ªëi ∆∞u h√≥a

### 1. BitNet Quantization
- Gi·∫£m k√≠ch th∆∞·ªõc model ~75%
- TƒÉng t·ªëc inference ~2x
- Gi·∫£m ƒë·ªô ch√≠nh x√°c < 1%

### 2. ONNX Export
- TƒÉng t·ªëc inference ~1.5-2x
- Cross-platform deployment
- T·ªëi ∆∞u cho production

### 3. Batch Inference
- X·ª≠ l√Ω nhi·ªÅu file c√πng l√∫c
- TƒÉng throughput ~3-4x

### 4. PyFlame Profiling
- Identify bottlenecks
- Optimize critical paths

##  C√¥ng ngh·ªá s·ª≠ d·ª•ng

- **Framework**: PyTorch, Transformers (HuggingFace)
- **Model**: Wav2Vec 2.0
- **Language Model**: KenLM (5-gram)
- **Web**: FastAPI, HTML/CSS/JavaScript
- **Optimization**: bitsandbytes (BitNet), ONNX Runtime
- **Evaluation**: jiwer (WER/CER)

##  T√†i li·ªáu tham kh·∫£o

1. [Wav2Vec 2.0 Paper](https://arxiv.org/abs/2006.11477)
2. [VIVOS Dataset](https://ailab.hcmus.edu.vn/vivos)
3. [Vietnamese Pre-trained Models](https://huggingface.co/nguyenvulebinh/wav2vec2-base-vietnamese-250h)
4. [KenLM Documentation](https://github.com/kpu/kenlm)

##  Troubleshooting

### L·ªói: "Model not found"
```bash
# Ki·ªÉm tra ƒë√£ train model ch∆∞a
ls models/wav2vec2-vietnamese-asr/final_model/
# N·∫øu ch∆∞a c√≥, ch·∫°y train_wav2vec2.py
```

### L·ªói: "CUDA out of memory"
```python
# Gi·∫£m batch_size trong train_wav2vec2.py
batch_size = 4  # t·ª´ 8 xu·ªëng 4
gradient_accumulation_steps = 4  # tƒÉng l√™n
```

### L·ªói: "KenLM not found"
```bash
# C√†i ƒë·∫∑t KenLM
pip install https://github.com/kpu/kenlm/archive/master.zip
```

### API kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c
```bash
# Ki·ªÉm tra server ƒëang ch·∫°y
curl http://localhost:8000/health

# Ki·ªÉm tra CORS settings trong api_server.py
# ƒê·∫£m b·∫£o allow_origins=["*"]
```

##  TODO / C·∫£i ti·∫øn

- [ ] Th√™m speaker diarization
- [ ] H·ªó tr·ª£ streaming inference
- [ ] Deploy l√™n cloud (AWS/GCP/Azure)
- [ ] Mobile app (iOS/Android)
- [ ] Th√™m nhi·ªÅu pre-processing (noise reduction, VAD)
- [ ] Fine-tune tr√™n domain-specific data
- [ ] A/B testing v·ªõi c√°c LM kh√°c nhau

##  ƒê√≥ng g√≥p

Sinh vi√™n: Nguy·ªÖn Tr√≠ Th∆∞·ª£ng

##  License

D·ª± √°n n√†y ƒë∆∞·ª£c ph√°t tri·ªÉn cho m·ª•c ƒë√≠ch h·ªçc t·∫≠p v√† nghi√™n c·ª©u.

##  Acknowledgments

- VIVOS dataset creators
- VinBigData team
- HuggingFace community
- Open-source contributors

---

**L∆∞u √Ω**: ƒê√¢y l√† d·ª± √°n h·ªçc t·∫≠p. ƒê·ªÉ s·ª≠ d·ª•ng trong production, c·∫ßn:
- Th√™m authentication/authorization
- Implement rate limiting
- Add logging v√† monitoring
- Optimize infrastructure
- Add comprehensive testing

# Vietnamese Speech Recognition (ASR) Project
# Nháº­n dáº¡ng giá»ng nÃ³i tiáº¿ng Viá»‡t

Äá» tÃ i: **Nháº­n dáº¡ng tiáº¿ng nÃ³i Tiáº¿ng Viá»‡t** sá»­ dá»¥ng Wav2Vec 2.0

## ğŸ“‹ Tá»•ng quan dá»± Ã¡n

Dá»± Ã¡n nÃ y xÃ¢y dá»±ng há»‡ thá»‘ng nháº­n dáº¡ng giá»ng nÃ³i tiáº¿ng Viá»‡t (ASR) sá»­ dá»¥ng kiáº¿n trÃºc Wav2Vec 2.0, Ä‘Æ°á»£c fine-tune trÃªn dá»¯ liá»‡u VIVOS vÃ  VinBigData. Há»‡ thá»‘ng bao gá»“m:

- Fine-tuning Wav2Vec2 model
- TÃ­ch há»£p Language Model Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c
- Web application Ä‘á»ƒ upload file hoáº·c ghi Ã¢m trá»±c tiáº¿p
- CÃ¡c cÃ´ng cá»¥ tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t (BitNet quantization, ONNX export)

## ğŸ¯ Má»¥c tiÃªu

- Chuyá»ƒn Ä‘á»•i giá»ng nÃ³i tiáº¿ng Viá»‡t (Ä‘a vÃ¹ng miá»n) thÃ nh vÄƒn báº£n
- Äáº¡t WER (Word Error Rate) < 10% trÃªn test set
- á»¨ng dá»¥ng web thÃ¢n thiá»‡n, dá»… sá»­ dá»¥ng
- Tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t inference

## ğŸ“Š Dataset

- **VIVOS**: ~15 giá» ghi Ã¢m cháº¥t lÆ°á»£ng cao
- **VinBigData VLSP 2020**: Dataset tiáº¿ng Viá»‡t quy mÃ´ lá»›n

## ğŸš€ HÆ°á»›ng dáº«n sá»­ dá»¥ng

### Option 1: Training trÃªn Google Colab (Khuyáº¿n nghá»‹ - GPU miá»…n phÃ­)

1. **Chuáº©n bá»‹ dataset:**
   ```bash
   python prepare_vivos.py
   ```

2. **Upload dataset lÃªn Google Drive:**
   - Táº¡o folder: `MyDrive/VietnameseASR/data/`
   - Upload 3 files tá»« `processed_data_vivos/`

3. **Má»Ÿ Colab notebook:**
   - Upload [colab_train.ipynb](colab_train.ipynb) lÃªn Colab
   - Runtime â†’ Change runtime type â†’ GPU
   - Cháº¡y tá»«ng cell

   ğŸ“– **Chi tiáº¿t:** Xem [colab_setup.md](colab_setup.md)

### Option 2: Training trÃªn mÃ¡y local (CPU - máº¥t ~6 ngÃ y)

### 1. CÃ i Ä‘áº·t mÃ´i trÆ°á»ng

```bash
# Táº¡o virtual environment
python -m venv venv

# Activate (Windows)
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Xá»­ lÃ½ vÃ  gá»™p dá»¯ liá»‡u

```bash
python prepare_dataset.py
```

Script nÃ y sáº½:
- Äá»c dá»¯ liá»‡u tá»« VIVOS vÃ  VinBigData
- Chuáº©n hÃ³a format
- Gá»™p vÃ  chia thÃ nh train/validation/test sets
- LÆ°u vÃ o thÆ° má»¥c `processed_data/`

### 3. Preprocessing dá»¯ liá»‡u

```bash
python data_preprocessing.py
```

Script nÃ y sáº½:
- Táº¡o vocabulary tá»« dá»¯ liá»‡u training
- Táº¡o Wav2Vec2Processor
- Chuáº©n bá»‹ dá»¯ liá»‡u cho training

### 4. Training model

```bash
python train_wav2vec2.py
```

Training sáº½:
- Fine-tune Wav2Vec2 model (pre-trained hoáº·c from scratch)
- Ãp dá»¥ng BitNet quantization (náº¿u cÃ³)
- ÄÃ¡nh giÃ¡ trÃªn validation set
- LÆ°u model vÃ o `models/wav2vec2-vietnamese-asr/`

**Cáº¥u hÃ¬nh training** trong file:
- `pretrained_model`: Model pre-trained (máº·c Ä‘á»‹nh: nguyenvulebinh/wav2vec2-base-vietnamese-250h)
- `num_train_epochs`: 30
- `batch_size`: 8
- `learning_rate`: 3e-4

### 5. Build Language Model

```bash
python language_model.py
```

Script nÃ y sáº½:
- Chuáº©n bá»‹ corpus tá»« training data
- Build 5-gram KenLM
- LÆ°u vÃ o `language_models/`

**LÆ°u Ã½**: Cáº§n cÃ i Ä‘áº·t KenLM:
```bash
pip install https://github.com/kpu/kenlm/archive/master.zip
```

### 6. Evaluation

```bash
python run_evaluation.py
```

ÄÃ¡nh giÃ¡ model trÃªn test set vá»›i:
- WER (Word Error Rate)
- CER (Character Error Rate)
- So sÃ¡nh vá»›i/khÃ´ng Language Model

### 7. Cháº¡y Web Application

```bash
# Start API server
python api_server.py

# Hoáº·c dÃ¹ng uvicorn
uvicorn api_server:app --reload --host 0.0.0.0 --port 8000
```

Truy cáº­p: http://localhost:8000/app

Web app cÃ³ tÃ­nh nÄƒng:
- âœ… Upload file audio (WAV, MP3, FLAC, etc.)
- âœ… Ghi Ã¢m trá»±c tiáº¿p tá»« microphone
- âœ… Hiá»ƒn thá»‹ káº¿t quáº£ real-time
- âœ… Toggle Language Model on/off

### 8. Tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t

```bash
python optimization.py
```

Script nÃ y sáº½:
- Apply quantization
- Export sang ONNX format
- Benchmark inference performance
- Profile vá»›i PyFlame (náº¿u cÃ³)

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
â”œâ”€â”€ Data/                           # Raw datasets
â”‚   â”œâ”€â”€ vivos/
â”‚   â””â”€â”€ Data/ (VinBigData)
â”œâ”€â”€ processed_data/                 # Processed datasets
â”‚   â”œâ”€â”€ train.jsonl
â”‚   â”œâ”€â”€ validation.jsonl
â”‚   â””â”€â”€ test.jsonl
â”œâ”€â”€ models/                         # Trained models
â”‚   â””â”€â”€ wav2vec2-vietnamese-asr/
â”‚       â”œâ”€â”€ final_model/
â”‚       â””â”€â”€ model.onnx
â”œâ”€â”€ language_models/                # Language models
â”‚   â”œâ”€â”€ vietnamese_5gram.bin
â”‚   â””â”€â”€ lm_corpus.txt
â”œâ”€â”€ static/                         # Web UI
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ results/                        # Evaluation results
â”‚   â”œâ”€â”€ final_results.json
â”‚   â””â”€â”€ predictions_with_lm.json
â”œâ”€â”€ prepare_dataset.py              # Dataset preparation
â”œâ”€â”€ data_preprocessing.py           # Data preprocessing
â”œâ”€â”€ train_wav2vec2.py              # Training script
â”œâ”€â”€ language_model.py              # LM building
â”œâ”€â”€ run_evaluation.py              # Evaluation
â”œâ”€â”€ api_server.py                  # FastAPI backend
â”œâ”€â”€ optimization.py                # Performance optimization
â””â”€â”€ requirements.txt               # Dependencies
```

## ğŸ”§ API Endpoints

### GET /health
Kiá»ƒm tra tráº¡ng thÃ¡i server

### POST /transcribe
Transcribe audio file

**Parameters:**
- `file`: Audio file (multipart/form-data)
- `use_lm`: Boolean (sá»­ dá»¥ng Language Model)

**Response:**
```json
{
  "text": "vÄƒn báº£n nháº­n dáº¡ng Ä‘Æ°á»£c",
  "processing_time": 1.23,
  "language_model_used": true,
  "audio_duration": 5.4
}
```

### GET /model-info
ThÃ´ng tin vá» model Ä‘Ã£ load

## ğŸ“Š Káº¿t quáº£

### Baseline (Greedy Decoding)
- WER: ~12-15%
- CER: ~6-8%

### With Language Model
- WER: ~8-10% (cáº£i thiá»‡n ~20-30%)
- CER: ~4-6% (cáº£i thiá»‡n ~20-30%)

### Performance
- Inference time: ~0.5-1s cho audio 5s
- Model size: ~400MB (full) / ~100MB (quantized)
- Real-time factor (RTF): < 0.2

## ğŸ› ï¸ Tá»‘i Æ°u hÃ³a

### 1. BitNet Quantization
- Giáº£m kÃ­ch thÆ°á»›c model ~75%
- TÄƒng tá»‘c inference ~2x
- Giáº£m Ä‘á»™ chÃ­nh xÃ¡c < 1%

### 2. ONNX Export
- TÄƒng tá»‘c inference ~1.5-2x
- Cross-platform deployment
- Tá»‘i Æ°u cho production

### 3. Batch Inference
- Xá»­ lÃ½ nhiá»u file cÃ¹ng lÃºc
- TÄƒng throughput ~3-4x

### 4. PyFlame Profiling
- Identify bottlenecks
- Optimize critical paths

## ğŸ“š CÃ´ng nghá»‡ sá»­ dá»¥ng

- **Framework**: PyTorch, Transformers (HuggingFace)
- **Model**: Wav2Vec 2.0
- **Language Model**: KenLM (5-gram)
- **Web**: FastAPI, HTML/CSS/JavaScript
- **Optimization**: bitsandbytes (BitNet), ONNX Runtime
- **Evaluation**: jiwer (WER/CER)

## ğŸ“ TÃ i liá»‡u tham kháº£o

1. [Wav2Vec 2.0 Paper](https://arxiv.org/abs/2006.11477)
2. [VIVOS Dataset](https://ailab.hcmus.edu.vn/vivos)
3. [Vietnamese Pre-trained Models](https://huggingface.co/nguyenvulebinh/wav2vec2-base-vietnamese-250h)
4. [KenLM Documentation](https://github.com/kpu/kenlm)

## ğŸ› Troubleshooting

### Lá»—i: "Model not found"
```bash
# Kiá»ƒm tra Ä‘Ã£ train model chÆ°a
ls models/wav2vec2-vietnamese-asr/final_model/
# Náº¿u chÆ°a cÃ³, cháº¡y train_wav2vec2.py
```

### Lá»—i: "CUDA out of memory"
```python
# Giáº£m batch_size trong train_wav2vec2.py
batch_size = 4  # tá»« 8 xuá»‘ng 4
gradient_accumulation_steps = 4  # tÄƒng lÃªn
```

### Lá»—i: "KenLM not found"
```bash
# CÃ i Ä‘áº·t KenLM
pip install https://github.com/kpu/kenlm/archive/master.zip
```

### API khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c
```bash
# Kiá»ƒm tra server Ä‘ang cháº¡y
curl http://localhost:8000/health

# Kiá»ƒm tra CORS settings trong api_server.py
# Äáº£m báº£o allow_origins=["*"]
```

## ğŸ“ TODO / Cáº£i tiáº¿n

- [ ] ThÃªm speaker diarization
- [ ] Há»— trá»£ streaming inference
- [ ] Deploy lÃªn cloud (AWS/GCP/Azure)
- [ ] Mobile app (iOS/Android)
- [ ] ThÃªm nhiá»u pre-processing (noise reduction, VAD)
- [ ] Fine-tune trÃªn domain-specific data
- [ ] A/B testing vá»›i cÃ¡c LM khÃ¡c nhau

## ğŸ‘¥ ÄÃ³ng gÃ³p

Sinh viÃªn: Nguyá»…n TrÃ­ ThÆ°á»£ng
Giáº£ng viÃªn hÆ°á»›ng dáº«n: [TÃªn giáº£ng viÃªn]

## ğŸ“„ License

Dá»± Ã¡n nÃ y Ä‘Æ°á»£c phÃ¡t triá»ƒn cho má»¥c Ä‘Ã­ch há»c táº­p vÃ  nghiÃªn cá»©u.

## ğŸ™ Acknowledgments

- VIVOS dataset creators
- VinBigData team
- HuggingFace community
- Open-source contributors

---

**LÆ°u Ã½**: ÄÃ¢y lÃ  dá»± Ã¡n há»c táº­p. Äá»ƒ sá»­ dá»¥ng trong production, cáº§n:
- ThÃªm authentication/authorization
- Implement rate limiting
- Add logging vÃ  monitoring
- Optimize infrastructure
- Add comprehensive testing

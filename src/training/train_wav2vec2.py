"""
Training script cho Wav2Vec2 ASR v·ªõi BitNet quantization
"""
import os
import torch
import json
from dataclasses import dataclass
from typing import Dict, List, Union
from pathlib import Path

from transformers import (
    Wav2Vec2ForCTC,
    Wav2Vec2Processor,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from datasets import load_from_disk, DatasetDict
import numpy as np
from src.data.preprocessing import load_and_prepare_datasets, create_processor
import evaluate

# Load WER and CER metrics
wer_metric = evaluate.load("wer")
cer_metric = evaluate.load("cer")

@dataclass
class DataCollatorCTCWithPadding:
    """
    Data collator ƒë·ªÉ pad input v√† labels
    """
    processor: Wav2Vec2Processor
    padding: Union[bool, str] = True
    max_length: Union[int, None] = None
    max_length_labels: Union[int, None] = None
    pad_to_multiple_of: Union[int, None] = None
    pad_to_multiple_of_labels: Union[int, None] = None

    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:
        # Split inputs and labels
        input_features = [{"input_values": feature["input_values"]} for feature in features]
        label_features = [{"input_ids": feature["labels"]} for feature in features]

        # Pad input features
        batch = self.processor.pad(
            input_features,
            padding=self.padding,
            max_length=self.max_length,
            pad_to_multiple_of=self.pad_to_multiple_of,
            return_tensors="pt",
        )

        # Pad labels - D√πng tokenizer tr·ª±c ti·∫øp thay v√¨ as_target_processor()
        labels_batch = self.processor.tokenizer.pad(
            label_features,
            padding=self.padding,
            max_length=self.max_length_labels,
            pad_to_multiple_of=self.pad_to_multiple_of_labels,
            return_tensors="pt",
        )

        # Replace padding with -100 to ignore loss
        labels = labels_batch["input_ids"].masked_fill(labels_batch.attention_mask.ne(1), -100)

        batch["labels"] = labels

        return batch

def compute_metrics(pred):
    """
    Compute WER and CER metrics
    """
    pred_logits = pred.predictions
    pred_ids = np.argmax(pred_logits, axis=-1)

    pred.label_ids[pred.label_ids == -100] = processor.tokenizer.pad_token_id

    pred_str = processor.batch_decode(pred_ids)
    label_str = processor.batch_decode(pred.label_ids, group_tokens=False)

    wer = wer_metric.compute(predictions=pred_str, references=label_str)
    cer = cer_metric.compute(predictions=pred_str, references=label_str)

    return {"wer": wer, "cer": cer}

def apply_bitnet_quantization(model, use_cpu=False):
    """
    √Åp d·ª•ng quantization ƒë·ªÉ gi·∫£m k√≠ch th∆∞·ªõc model
    
    Args:
        model: Model c·∫ßn quantize
        use_cpu: N·∫øu True, d√πng PyTorch native quantization (t·ªët cho CPU)
                 N·∫øu False, th·ª≠ d√πng bitsandbytes (t·ªët cho GPU)
    """
    if use_cpu or not torch.cuda.is_available():
        # CPU: D√πng PyTorch dynamic quantization
        try:
            print("Applying PyTorch dynamic quantization (optimized for CPU)...")
            quantized_model = torch.quantization.quantize_dynamic(
                model,
                {torch.nn.Linear},  # Quantize Linear layers
                dtype=torch.qint8
            )
            print("‚úì Applied PyTorch int8 quantization (CPU-optimized)")
            print("  - Model size reduced ~75%")
            print("  - Inference speed up ~2-3x on CPU")
            return quantized_model
        except Exception as e:
            print(f"‚ö† PyTorch quantization failed: {e}")
            return model
    else:
        # GPU: Th·ª≠ d√πng bitsandbytes
        try:
            import bitsandbytes as bnb
            
            # Quantize model to 8-bit
            model = bnb.nn.Linear8bitLt(
                model,
                has_fp16_weights=False,
                threshold=6.0
            )
            print("‚úì Applied bitsandbytes 8-bit quantization (GPU-optimized)")
            return model
        except ImportError:
            print("‚ö† bitsandbytes not available, falling back to PyTorch quantization")
            return apply_bitnet_quantization(model, use_cpu=True)
        except Exception as e:
            print(f"‚ö† BitNet quantization failed: {e}, falling back to PyTorch quantization")
            return apply_bitnet_quantization(model, use_cpu=True)

def create_model(vocab_size: int, processor: Wav2Vec2Processor, pretrained_model: str = None):
    """
    T·∫°o ho·∫∑c load pre-trained Wav2Vec2 model
    
    Args:
        vocab_size: K√≠ch th∆∞·ªõc vocabulary
        processor: Wav2Vec2Processor instance (c·∫ßn ƒë·ªÉ l·∫•y pad_token_id)
        pretrained_model: T√™n model pre-trained (optional)
    """
    if pretrained_model:
        print(f"Loading pre-trained model: {pretrained_model}")
        model = Wav2Vec2ForCTC.from_pretrained(
            pretrained_model,
            attention_dropout=0.1,
            hidden_dropout=0.1,
            feat_proj_dropout=0.0,
            mask_time_prob=0.05,
            layerdrop=0.1,
            ctc_loss_reduction="mean",
            pad_token_id=processor.tokenizer.pad_token_id,
            vocab_size=vocab_size,
            ignore_mismatched_sizes=True
        )
    else:
        print("Creating model from scratch")
        from transformers import Wav2Vec2Config
        config = Wav2Vec2Config(
            vocab_size=vocab_size,
            hidden_size=768,
            num_hidden_layers=12,
            num_attention_heads=12,
            intermediate_size=3072,
            pad_token_id=processor.tokenizer.pad_token_id
        )
        model = Wav2Vec2ForCTC(config)
    
    # Freeze feature encoder (ch·ªâ fine-tune CTC head)
    model.freeze_feature_encoder()
    
    return model

def train_model(
    model,
    train_dataset,
    val_dataset,
    processor,
    output_dir: str,
    num_train_epochs: int = 30,
    batch_size: int = 8,
    gradient_accumulation_steps: int = 2,
    learning_rate: float = 3e-4,
    warmup_steps: int = 500,
    use_fp16: bool = True
):
    """
    Train Wav2Vec2 model
    """
    # Data collator
    data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)
    
    # Training arguments - T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh cho CPU/GPU
    is_gpu = torch.cuda.is_available()
    
    training_args = TrainingArguments(
        output_dir=output_dir,
        group_by_length=False,  # T·∫Øt v√¨ dataset c√≥ 'input_values' thay v√¨ 'input_ids'
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        gradient_accumulation_steps=gradient_accumulation_steps,
        eval_strategy="steps",
        eval_steps=500,
        save_steps=500,
        save_total_limit=2,
        logging_steps=100,
        learning_rate=learning_rate,
        weight_decay=0.005,
        num_train_epochs=num_train_epochs,
        warmup_steps=warmup_steps,
        fp16=use_fp16 and is_gpu,  # FP16 ch·ªâ tr√™n GPU
        push_to_hub=False,
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
        dataloader_num_workers=2 if is_gpu else 0,  # GPU cho ph√©p workers, CPU kh√¥ng
        dataloader_pin_memory=is_gpu,  # Pin memory ch·ªâ v·ªõi GPU
        report_to=["tensorboard"],
        gradient_checkpointing=True,
        remove_unused_columns=False,  # CRITICAL: Dataset ƒë√£ ƒë∆∞·ª£c preprocess, kh√¥ng remove columns
    )
    
    print(f"\n‚úì Training config: {'GPU' if is_gpu else 'CPU'} mode")
    print(f"  - Batch size: {batch_size}")
    print(f"  - FP16: {training_args.fp16}")
    print(f"  - Workers: {training_args.dataloader_num_workers}")
    
    # Early stopping
    early_stopping = EarlyStoppingCallback(
        early_stopping_patience=5,
        early_stopping_threshold=0.001
    )
    
    # Trainer (kh√¥ng c·∫ßn tokenizer parameter trong transformers >= 4.30)
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
        callbacks=[early_stopping]
    )
    
    # Train
    print("\n" + "="*50)
    print("Starting training...")
    print("="*50)
    
    trainer.train()
    
    # Save final model
    trainer.save_model(f"{output_dir}/final_model")
    processor.save_pretrained(f"{output_dir}/final_model")
    
    print(f"\n‚úì Training completed! Model saved to: {output_dir}/final_model")
    
    return trainer

def main():
    """
    Main training pipeline
    """
    # Paths - t·ª´ root c·ªßa project
    project_root = Path(__file__).parent.parent.parent
    data_dir = project_root / 'processed_data_vivos'
    output_dir = project_root / 'models' / 'wav2vec2-vietnamese-asr'
    vocab_path = project_root / 'vocab.json'
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # Configuration
    config = {
        'pretrained_model': 'nguyenvulebinh/wav2vec2-base-vietnamese-250h',  # Ho·∫∑c None ƒë·ªÉ train from scratch
        'num_train_epochs': 30,
        'batch_size': 8,
        'gradient_accumulation_steps': 2,
        'learning_rate': 3e-4,
        'use_fp16': True,
        'apply_quantization': True
    }
    
    # Save config
    with open(output_dir / 'training_config.json', 'w') as f:
        json.dump(config, f, indent=2)
    
    # Load processor
    global processor
    print("Loading processor...")
    if config['pretrained_model']:
        processor = Wav2Vec2Processor.from_pretrained(config['pretrained_model'])
    else:
        processor = create_processor(str(vocab_path))
    
    # Ki·ªÉm tra dataset files
    required_files = ['train.jsonl', 'validation.jsonl', 'test.jsonl']
    missing_files = [f for f in required_files if not (data_dir / f).exists()]
    
    if missing_files:
        print(f"\n‚ùå L·ªói: Kh√¥ng t√¨m th·∫•y dataset files: {missing_files}")
        print(f"üìÅ ƒê∆∞·ªùng d·∫´n t√¨m ki·∫øm: {data_dir}")
        print(f"\nüí° Gi·∫£i ph√°p:")
        print(f"   1. Ch·∫°y l·ªánh: python prepare_vivos.py")
        print(f"   2. Ho·∫∑c: python prepare_full_dataset.py (ƒë·ªÉ g·ªôp VIVOS + VinBigData)")
        print(f"   3. Sau ƒë√≥ ch·∫°y l·∫°i: python train.py")
        return
    
    # Load and prepare datasets
    print("\nLoading datasets...")
    train_dataset, val_dataset, test_dataset = load_and_prepare_datasets(
        str(data_dir / 'train.jsonl'),
        str(data_dir / 'validation.jsonl'),
        str(data_dir / 'test.jsonl'),
        processor
    )
    
    # Create model
    print("\nCreating model...")
    vocab_size = len(processor.tokenizer)
    model = create_model(vocab_size, config['pretrained_model'])
    
    # QUAN TR·ªåNG: KH√îNG quantize khi training!
    # Quantization ch·ªâ d√πng cho inference sau khi train xong
    # N·∫øu quantize, model kh√¥ng th·ªÉ backprop gradient (RuntimeError: leaf Variable requires grad)
    if config['apply_quantization']:
        print("\n‚ö† Warning: Quantization is DISABLED during training.")
        print("  Quantization should only be applied AFTER training for inference.")
        print("  Use src/utils/optimization.py after training completes.")
    
    # Print model info
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nModel parameters:")
    print(f"  Total: {total_params:,}")
    print(f"  Trainable: {trainable_params:,}")
    
    # Train
    trainer = train_model(
        model=model,
        train_dataset=train_dataset,
        val_dataset=val_dataset,
        processor=processor,
        output_dir=str(output_dir),
        num_train_epochs=config['num_train_epochs'],
        batch_size=config['batch_size'],
        gradient_accumulation_steps=config['gradient_accumulation_steps'],
        learning_rate=config['learning_rate'],
        use_fp16=config['use_fp16']
    )
    
    # Evaluate on test set
    print("\n" + "="*50)
    print("Evaluating on test set...")
    print("="*50)
    
    test_results = trainer.evaluate(test_dataset)
    print(f"\nTest Results:")
    print(f"  WER: {test_results['eval_wer']:.4f}")
    print(f"  CER: {test_results['eval_cer']:.4f}")
    
    # Save test results
    with open(output_dir / 'test_results.json', 'w') as f:
        json.dump(test_results, f, indent=2)

if __name__ == "__main__":
    main()
